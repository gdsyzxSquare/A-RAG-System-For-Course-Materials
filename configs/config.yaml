# RAG System Configuration

# Data Paths
data:
  raw_data_path: "data/raw/course_documents_cleaned.txt"
  processed_data_path: "data/processed/chunks.json"
  embeddings_path: "data/embeddings/faiss_index"
  eval_dataset_path: "evaluation/eval_dataset.json"

# Chunking Strategy
chunking:
  strategy: "sliding_window"  # Options: fixed_size, semantic, sliding_window
  chunk_size: 512
  chunk_overlap: 128  # Increased overlap for better coverage
  separators: ["\n\n", "\n", ". ", " "]
  extract_metadata: true  # Extract metadata and create dedicated chunks

# Embedding Model
embedding:
  model_name: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"  # Q&A optimized model
  batch_size: 32
  max_length: 512

# Retrieval
retrieval:
  method: "hybrid"  # Options: dense, bm25, hybrid
  top_k: 10
  similarity_threshold: 0.3
  
# BM25 Settings
bm25:
  k1: 1.5
  b: 0.75

# Re-ranker (Optional)
reranker:
  enabled: true
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 5  # Keep top 7 chunks after re-ranking

# LLM Configuration
llm:
  provider: "deepseek"  # Options: openai, deepseek, anthropic, local
  model_name: "deepseek-chat"  # deepseek-chat or deepseek-coder
  temperature: 0.0
  max_tokens: 1024
  api_key_env: "DEEPSEEK_API_KEY"
  base_url: "https://api.deepseek.com"  # Optional: custom endpoint

# Prompts
prompts:
  rag_prompt: |
    You are a helpful teaching assistant. Answer the question based ONLY on the provided context from course materials.
    Read ALL the context passages carefully before answering.
    If the answer is in ANY of the passages, provide it. If not, say "I don't know based on the provided materials."
    However, when the question is creative or open-ended, you may integrate your own knowledge to provide a comprehensive answer.
    Context:
    {context}
    
    Question: {question}
    
    Answer:
  
  closed_book_prompt: |
    Answer the following question about the course:
    
    Question: {question}
    
    Answer:

# Evaluation
evaluation:
  metrics: ["recall@k", "mrr", "exact_match", "f1", "rouge"]
  recall_at: [1, 3, 5]
  run_closed_book: false  # Whether to run closed-book evaluation (no RAG context)
  llm_judge:
    enabled: true  # Enable LLM-as-judge evaluation (costs API calls)
    criteria: ["correctness", "overall"]  # Evaluation criteria
    # Options: faithfulness, relevance, completeness, correctness, overall

# Advanced Features
advanced:
  query_rewriting:
    enabled: true
    method: "dense"  # Options: simple, hyde, dense (simple now optimized!)
  
  caching:
    enabled: true
    cache_path: "data/cache"
  
  profiling:
    enabled: false
    log_latency: true
    log_memory: true

# Experiment Tracking
experiment:
  name: "baseline_rag"
  results_dir: "results"
  save_predictions: true
