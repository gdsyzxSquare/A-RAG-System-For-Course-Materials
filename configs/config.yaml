# RAG System Configuration

# Data Paths
data:
  raw_data_path: "data/raw/course_documents_cleaned.txt"
  processed_data_path: "data/processed/chunks.json"
  embeddings_path: "data/embeddings/faiss_index"
  eval_dataset_path: "evaluation/eval_dataset.json"

# Chunking Strategy
chunking:
  strategy: "fixed_size"  # Options: fixed_size, semantic, sliding_window
  chunk_size: 512
  chunk_overlap: 50
  separators: ["\n\n", "\n", ". ", " "]

# Embedding Model
embedding:
  model_name: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"  # Q&A optimized model
  batch_size: 32
  max_length: 512

# Retrieval
retrieval:
  method: "hybrid"  # Options: dense, bm25, hybrid
  top_k: 10
  similarity_threshold: 0.5
  
# BM25 Settings
bm25:
  k1: 1.5
  b: 0.75

# Re-ranker (Optional)
reranker:
  enabled: true
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 3

# LLM Configuration
llm:
  provider: "deepseek"  # Options: openai, deepseek, anthropic, local
  model_name: "deepseek-chat"  # deepseek-chat or deepseek-coder
  temperature: 0.0
  max_tokens: 1024
  api_key_env: "DEEPSEEK_API_KEY"
  base_url: "https://api.deepseek.com"  # Optional: custom endpoint

# Prompts
prompts:
  rag_prompt: |
    You are a helpful teaching assistant. Answer the question based ONLY on the provided context from course materials.
    Read ALL the context passages carefully before answering.
    If the answer is in ANY of the passages, provide it. If not, say "I don't know based on the provided materials."
    
    Context:
    {context}
    
    Question: {question}
    
    Answer:
  
  closed_book_prompt: |
    Answer the following question about the course:
    
    Question: {question}
    
    Answer:

# Evaluation
evaluation:
  metrics: ["recall@k", "mrr", "exact_match", "f1", "rouge"]
  recall_at: [1, 3, 5]
  llm_judge:
    enabled: true
    sample_size: 30
    criteria: ["faithfulness", "relevance", "completeness"]

# Advanced Features
advanced:
  query_rewriting:
    enabled: true
    method: "hyde"  # Options: hyde, condense
  
  caching:
    enabled: true
    cache_path: "data/cache"
  
  profiling:
    enabled: false
    log_latency: true
    log_memory: true

# Experiment Tracking
experiment:
  name: "baseline_rag"
  results_dir: "results"
  save_predictions: true
