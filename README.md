# Course RAG System

A production-ready Retrieval-Augmented Generation (RAG) system for answering questions based on course documents, featuring metadata extraction, re-ranking, and LLM-as-judge evaluation.

## âœ¨ Key Features

- ğŸ” **Hybrid Retrieval**: Dense embeddings + BM25 fusion
- ğŸ¯ **Re-ranking**: Cross-encoder for improved relevance
- ğŸ“Š **Metadata Extraction**: Dedicated chunks for structured info (instructor, location, time, etc.)
- ğŸ”„ **Query Rewriting**: Pattern-based query optimization
- ğŸ¤– **LLM-as-Judge**: Multi-criteria answer quality evaluation
- ğŸ“ˆ **Comprehensive Metrics**: F1, ROUGE, Recall@k, MRR, and more

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ configs/              # Configuration files
â”‚   â””â”€â”€ config.yaml      # Main configuration (supports all features)
â”œâ”€â”€ data/                # Data directory
â”‚   â”œâ”€â”€ raw/            # Raw course documents
â”‚   â””â”€â”€ embeddings/     # Vector embeddings (FAISS index)
â”œâ”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ data_loader.py  # Data loading and preprocessing
â”‚   â”œâ”€â”€ chunking.py     # Multiple strategies + metadata extraction
â”‚   â”œâ”€â”€ embeddings.py   # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py # FAISS vector store
â”‚   â”œâ”€â”€ retriever.py    # BM25, Dense, and Hybrid retrievers
â”‚   â”œâ”€â”€ rag_pipeline.py # Complete RAG pipeline with re-ranker
â”‚   â””â”€â”€ query_rewriter.py # Query rewriting strategies
â”œâ”€â”€ evaluation/         # Evaluation modules
|   â”œâ”€â”€ eval_dataset.json  # Evaluation dataset
â”‚   â”œâ”€â”€ metrics.py      # Evaluation metrics
â”‚   â”œâ”€â”€ evaluation.py   # Enhanced evaluation framework
â”‚   â””â”€â”€ llm_judge.py    # âœ¨ LLM-as-Judge implementation
â”œâ”€â”€ results/            # Evaluation results
â”œâ”€â”€ main.py            # Main entry point (supports all modes)
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ clean_data         # Clean raw data
â””â”€â”€ README.md         # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup

```bash
# Create .env file with your API keys
# Example:
# DASHSCOPE_API_KEY=your_dashscope_key_here
# MOONSHOT_API_KEY=your_moonshot_key_here
```

### 3. Basic Usage

```bash
# Complete pipeline: preprocess â†’ build â†’ query
python clean_data.py # Clean data placed in data/raw/course_documents.txt
python main.py --mode build       # Build vector store with embeddings
python main.py --mode query       # Interactive Q&A mode

# Evaluation with full features (LLM Judge + all metrics)
python main.py --mode evaluate    # Uses config.yaml settings
```

**âš¡ Evaluation Tips**:
- Configure evaluation features in `configs/config.yaml`
- Set `run_closed_book: false` to skip closed-book eval (saves ~50% time/cost)
- Configure `llm_judge.criteria` to customize LLM evaluation

### 4. Prepare Data

Place your course documents (plain text extracted from PPT) in:
```
data/raw/course_documents.txt
```
run
```
python clean_data.py
```
to clean data
### 5. Build Index & Query

```bash
# Build vector store from documents
python main.py --mode build

# Query interactively
python main.py --mode query
```

The system will:
- Extract metadata
- Chunk documents
- Generate embeddings and create FAISS index
- Enable hybrid retrieval with re-ranking (optional)

### 6. Evaluation

Create an evaluation dataset at `evaluation/eval_dataset.json`:

```json
[
  {
    "question": "What is the main topic?",
    "answer": "The main topic is...",
    "relevant_chunks": [0, 1, 2],
    "metadata": {"difficulty": "easy"}
  }
]
```

Then run:

```bash
python main.py --mode evaluate
```

This generates a comprehensive evaluation report with:
- ğŸ“Š **Standard Metrics**: Recall@k, MRR, F1, ROUGE-L
- ğŸ¤– **LLM-as-Judge** (optional): Multi-criteria quality evaluation  
- ğŸ”„ **Closed-book Comparison** (optional): RAG vs no retrieval

**Tip**: Configure evaluation features in `configs/config.yaml` - see [Configuration](#-configuration) section below.

## ğŸ“ Configuration

The system is configured via `configs/config.yaml`:


## ğŸ¤– LLM-as-Judge Evaluation

The system includes LLM-based answer quality evaluation across multiple criteria.

## ğŸ“ˆ Metadata Extraction

The system automatically extracts structured metadata from course documents:

**Extracted Fields**:
- ğŸ“ Location: Course venue
- ğŸ• Time: Class schedule
- ğŸ‘¨â€ğŸ« Instructor: Name & contact
- ğŸ“Š Course Info: Code, title, credits
- ğŸ“š Prerequisites: Required background
- ğŸ¯ Learning Objectives
- ğŸ“– Textbooks

## ğŸ“Š Project Requirements Checklist

### Core Objectives âœ…
- [x] Data cleaning and preprocessing
- [x] Chunking strategies (fixed-size, semantic, sliding window)
- [x] Metadata extraction (13 metadata chunks)
- [x] Embedding generation (sentence-transformers)
- [x] Vector index (FAISS)
- [x] Retriever (BM25 + Dense + Hybrid)
- [x] Re-ranking (Cross-encoder: 10 â†’ 5 chunks)
- [x] Generator (LLM integration)
- [x] Evaluation framework
  - [x] Retrieval metrics (Recall@k, MRR)
  - [x] Answer metrics (Exact Match, F1, ROUGE)
  - [x] **LLM-as-Judge** (5 criteria, configurable)
  - [x] **Configurable evaluation** (closed-book toggle, criteria selection)

### Comparison Experiments ğŸ”„
- [x] Closed-book vs RAG (**configurable via config.yaml**)
- [x] Compare retrievers (BM25 vs Dense vs Hybrid)
- [x] Compare prompts (modify config.yaml)

### Advanced Features ğŸ¯
- [x] Query rewriting (pattern-based optimization)
- [x] Re-ranking (Cross-encoder: top 10 â†’ top 5)
- [x] Metadata extraction (13 specialized chunks)
- [x] LLM-as-Judge evaluation (multi-criteria)

