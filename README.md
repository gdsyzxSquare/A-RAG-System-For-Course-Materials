# Course RAG System

A Retrieval-Augmented Generation (RAG) system for answering questions based on course documents (PPT lectures).

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ configs/              # Configuration files
â”‚   â””â”€â”€ config.yaml      # Main configuration
â”œâ”€â”€ data/                # Data directory
â”‚   â”œâ”€â”€ raw/            # Raw course documents
â”‚   â”œâ”€â”€ processed/      # Processed chunks
â”‚   â””â”€â”€ embeddings/     # Vector embeddings
â”œâ”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ data_loader.py  # Data loading and preprocessing
â”‚   â”œâ”€â”€ chunking.py     # Document chunking strategies
â”‚   â”œâ”€â”€ embeddings.py   # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py # FAISS vector store
â”‚   â”œâ”€â”€ retriever.py    # BM25 and dense retrievers
â”‚   â””â”€â”€ rag_pipeline.py # Complete RAG pipeline
â”œâ”€â”€ evaluation/         # Evaluation modules
â”‚   â”œâ”€â”€ metrics.py      # Evaluation metrics
â”‚   â””â”€â”€ evaluation.py   # Evaluation framework
â”œâ”€â”€ notebooks/          # Jupyter notebooks
â”‚   â””â”€â”€ 01_rag_pipeline_demo.md
â”œâ”€â”€ results/            # Evaluation results
â”œâ”€â”€ main.py            # Main entry point
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md         # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup

```bash
# Copy environment template
copy .env.example .env

# Edit .env and add your DeepSeek API key (recommended)
# DEEPSEEK_API_KEY=sk-your-key-here
# 
# Or use OpenAI API key:
# OPENAI_API_KEY=your-key-here
```

**æ¨èä½¿ç”¨ DeepSeek API**:
- ğŸ’° ä»·æ ¼å®æƒ ï¼ˆè¿œä½äºOpenAIï¼‰
- ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ”¯æŒå¥½
- âš¡ å“åº”é€Ÿåº¦å¿«
- ğŸ“– æŸ¥çœ‹ `DEEPSEEK_SETUP.md` è·å–è¯¦ç»†é…ç½®æŒ‡å—

### 2.5 Test API Connection

```bash
# Test DeepSeek API
python test_deepseek.py
```

è¿™ä¼šéªŒè¯ä½ çš„APIé…ç½®æ˜¯å¦æ­£ç¡®ã€‚

### 3. Prepare Data

Place your course documents (plain text extracted from PPT) in:
```
data/raw/course_documents.txt
```

### 4. Build Index

```bash
python main.py --mode build
```

This will:
- Load and preprocess your documents
- Chunk them into smaller pieces
- Generate embeddings
- Create a FAISS vector index

### 5. Query the System

```bash
python main.py --mode query
```

Interactive mode where you can ask questions about your course materials.

### 6. Evaluate

First, create an evaluation dataset at `evaluation/eval_dataset.json`:

```json
[
  {
    "question": "What is the main topic?",
    "answer": "The main topic is...",
    "relevant_chunks": [0, 1, 2],
    "metadata": {"difficulty": "easy"}
  }
]
```

Then run evaluation:

```bash
python main.py --mode evaluate
```

## ğŸ“Š Project Requirements Checklist

### Core Objectives âœ…
- [x] Data cleaning and preprocessing
- [x] Chunking strategies (fixed-size, semantic, sliding window)
- [x] Embedding generation (sentence-transformers)
- [x] Vector index (FAISS)
- [x] Retriever (BM25 + Dense)
- [x] Generator (LLM integration)
- [x] Evaluation framework
  - [ ] Create evaluation dataset (â‰¥50 samples) - **YOUR TASK**
  - [x] Retrieval metrics (Recall@k, MRR)
  - [x] Answer metrics (Exact Match, F1, ROUGE)
  - [ ] LLM-as-Judge (â‰¥30 samples) - **TODO**

### Comparison Experiments ğŸ”„
- [ ] Closed-book vs RAG - **Ready to run after eval dataset**
- [ ] Compare retrievers (BM25 vs Dense vs Hybrid) - **Implemented**
- [ ] Compare prompts - **Modify config.yaml**

### Advanced Features (Choose â‰¥2) ğŸ¯
- [ ] Query rewriting (HyDE)
- [ ] Re-ranking (Cross-encoder)
- [ ] Latency & memory profiling
- [ ] Your own variant

## ğŸ“ Next Steps

1. **Create evaluation dataset** (Priority 1)
   - Design at least 50 questions from your course materials
   - Include ground truth answers
   - Specify relevant chunk IDs
   - Save to `evaluation/eval_dataset.json`

2. **Run baseline experiments**
   - Build index with different chunking strategies
   - Compare retrievers (modify config.yaml)
   - Test different prompts

3. **Implement advanced features**
   - Choose at least 2 from the list
   - Document your implementation

4. **Analysis and reporting**
   - Generate comparison charts
   - Write findings
   - Document limitations

## ğŸ› ï¸ Configuration

Edit `configs/config.yaml` to customize:
- Chunking strategy and size
- Embedding model
- Retrieval method
- LLM provider and model
- Prompts

## ğŸ“š Key Files to Understand

- `src/chunking.py` - Different chunking strategies
- `src/retriever.py` - BM25, Dense, and Hybrid retrieval
- `src/rag_pipeline.py` - End-to-end RAG system
- `evaluation/metrics.py` - Evaluation metrics implementation

## âš ï¸ Important Notes

1. **Import errors** shown by VS Code are normal - packages will be installed via requirements.txt
2. **API keys** required for LLM generation (OpenAI or Anthropic)
3. **Evaluation dataset** must be created manually based on your course content
4. **At least 50 evaluation samples** required per project requirements

## ğŸ“§ Support

For questions about the project requirements, refer to `requirment.txt`.

---

**Remember**: This is a framework. You need to:
1. Add your course documents
2. Create evaluation dataset
3. Run experiments
4. Implement advanced features
5. Analyze and report results

Good luck! ğŸ“
